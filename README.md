# __NLP__

I. Base_Line.ipynb
-
  - Обучение и валидация

    Стандартно для правильной валидации модели используют отложенную выборку. То есть мы разбиваем наши данные на тренировочную выборку, тестовую выборку и отложенную выборку. Соответственно, обучаем модель на тренировочной, в ходе обучения проверяем результат на тестовой выборке, а в конце обучения, чтобы оценить качество модели, ошибку считаем на отложенной выборке.

    ![image](https://user-images.githubusercontent.com/41290137/117573756-5e87fd00-b0e2-11eb-855c-da7f5a1c0deb.png)

    Однако, при таком подходе в обучении модели участвует только тренировочная выборка. Тестовую и отложенную мы используем только для проверки. Если у нас мало данных - это непозволительная роскошь. Другой популярный подход это кросс-валидация или скользящий контроль. Суть метода заключается в том, что мы делаем не одно разбиение датесета, а несколько разбиений таким образом, чтобы все данные использовались и в обучении и для проверки.

    ![image](https://user-images.githubusercontent.com/41290137/117573783-7c556200-b0e2-11eb-9266-1811e3aaa196.png)

    Преимущества такого подхода:
      - используем все данные для обучения;
      - можем оценить устойчивость модели. Если ошибки полученные на разных фолдах сильно отличаются, то модель неустойчива.
    
    Недостаток: 
      - нужно обучать не одну модель, а несколько (столько, сколько мы выбрали фолдов).


    Функция cross_val_score воспроизводит разбиение, обучение и тестирование в соответствие с типом и параметрами передаваемого в нее валидатора. В нее передаем оцениваемую модель, таблицу входных данных, выходную переменную, способ разделения данных (фолды) и метрику, которую мы хотим оценить. В данном случае мы хотим оценить r2_score. На выходе получим значения метрик. Так как мы передали в KFold с параметром n_splits=5, то и значений мы получим 5.

  - Поиск по сетке

    Теперь, когда у нас есть надёжный способ оценивать качество модели, мы можем перейти к подбору гиперпараметров модели. Чтобы выработать некоторую интуицию о самых важных параметрах градиентного бустинга на решающих деревьях, сначала мы попробуем в ручную поменять их и посмотреть, как меняются метрики. 

    Вместо того, чтобы перебирать параметры руками, можно использовать метод поиска по сетке (Grid Search). В процессе поиска по сетке мы указываем варианты каждого из параметров, которые хотим перебрать, а функция смотрит на все их возможные варианты и выдает лучший набор в зависимости от выбранной метрики. Например, на картинке ниже перебираются параметры "регуляризация" и "скорость обучения".

    ![image](https://user-images.githubusercontent.com/41290137/117573886-2503c180-b0e3-11eb-84f7-286d7e9ad2c0.png)


II. RNN_LSTM_model.ipynb
-
  - Здесь рассматриваются два подхода: RNN

  ![image](https://user-images.githubusercontent.com/41290137/117574170-84ae9c80-b0e4-11eb-994e-897c5d26fcc9.png)
  
  
  
  
  - и LSTM



  ![image](https://user-images.githubusercontent.com/41290137/117574222-c5a6b100-b0e4-11eb-9f5d-bd7105440bba.png)
  ![image](https://user-images.githubusercontent.com/41290137/117574238-d820ea80-b0e4-11eb-8f5e-f091640ed65b.png)

III. Transf_pretrained.ipynb
-
  - Предобученная модель

  Одиночные блоки RNN и LSTM не могут вытащить достаточно информации для хороших предсказаний. При этом измерить в лаборатории молекулярные свойста для большого числа молекул - дорогой по времени и материалам процесс. Есть датасеты известных молекул состоящие из миллионов объектов. Один из таких корпусов - ChemBL database.
  Что если мы попробуем вытащить скрытые свойства молекул имея на руках только SMILES, не используя предсказания растворимости? - Это делается при помощи моделей автоэнкодеров.
  
  ![image](https://user-images.githubusercontent.com/41290137/117574460-d9064c00-b0e5-11eb-8525-0568593157ee.png)
  
  - Transformer
  
  В моделях, работающих с естественным языком SOTA моделями являются трансформеры. Трансформеры включают в себя слои Attention - это слои, которые сохраняют дополнительную информацию о слове, которое они обрабатывают. Когда мы читаем предложения в тексте - мы обращаем внимание на конкретные слова. Attention реализуют внутри себя эту функцию. Передавая "важность" слова между слоями мы можем улучшить наши предсказания. Трансформер состоит из нескольких блоков, включающих слои Attention.
  
  ![image](https://user-images.githubusercontent.com/41290137/117574545-4a45ff00-b0e6-11eb-8530-05b1d035b53c.png)

